{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from nltk import tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql.functions import row_number,lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas\n",
    "from pyspark.sql import SQLContext\n",
    "import rouge\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"text_rank\").getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sql = SQLContext(sc)\n",
    "datafile = pandas.read_csv('dataset.csv')\n",
    "data = sql.createDataFrame(datafile)\n",
    "content = data.select('CONTENT')\n",
    "w = Window().orderBy(lit('A'))\n",
    "df = content.withColumn(\"row_num\", row_number().over(w))\n",
    "rdd = df.rdd.map(list)\n",
    "items = rdd.count()\n",
    "NoOfIterations = 20\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateNode(file):\n",
    "    sentence_id = file[1]\n",
    "    sentences = file[0].split(\".\")\n",
    "    output = []\n",
    "    for index, sent in enumerate(sentences):\n",
    "        sent_id = str(sentence_id) + '_' + str(index)\n",
    "        sent_len = len(sent.split(\" \"))\n",
    "        if 10 < sent_len < 30:\n",
    "            output.append((sent_id, sent))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FilterNode(sent):\n",
    "    sentence_id, sentence = sent[0], sent[1]\n",
    "    lmtz = WordNetLemmatizer()\n",
    "    result = []\n",
    "    output = []\n",
    "    words = re.findall(r'[a-zA-Z]+', str(sentence))\n",
    "    for w in words:\n",
    "        if w.lower() not in sw:\n",
    "            w = lmtz.lemmatize(w.lower())\n",
    "            result.append(w)\n",
    "    for w in result:\n",
    "        if len(w) > 3:\n",
    "            word = w\n",
    "            output.append(word)\n",
    "    return sentence_id, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdjecencyList(node, totalnodes):\n",
    "    n,v = node[0],node[1]\n",
    "    edges = {}\n",
    "    for u in totalnodes:\n",
    "        edge = Relation(node, u)\n",
    "        if edge is not None:\n",
    "            edges[edge[0]] = edge[1]\n",
    "    return (n, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relation(node1, node2):\n",
    "    n1,v1 = node1[0],node1[1]\n",
    "    n2,v2 = node2[0],node2[1]\n",
    "    if n1 != n2: \n",
    "        n_len = len(set(v1).intersection(v2))\n",
    "        log_len = np.log2(len(v1)) + np.log2(len(v2))\n",
    "        if log_len == 0:\n",
    "            coefficient = n_len/(log_len+1)\n",
    "        else:\n",
    "            coefficient = n_len/(log_len)\n",
    "        if coefficient != 0:\n",
    "            return (n2, coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Contribution(neighbours, rank):\n",
    "    output = []\n",
    "    totalweight = sum(neighbours.values())\n",
    "    for key,weight in neighbours.items():\n",
    "        weightage = (rank*weight)/totalweight\n",
    "        output.append((key, weightage))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Summary(text):\n",
    "    file = CreateNode(text)\n",
    "    textfile = sc.parallelize(file)\n",
    "    nodes = textfile.map(lambda l: FilterNode(l))\n",
    "    textfile = textfile.cache()\n",
    "    total_nodes = nodes.collect()\n",
    "    textrank_graph = nodes.map(lambda ver: AdjecencyList(ver, total_nodes))\n",
    "    textrank_graph = textrank_graph.filter(lambda l: len(l[1]) > 0).cache()\n",
    "    rank_rdd = textrank_graph.map(lambda x: (x[0],0.15))\n",
    "    collection = textrank_graph.join(rank_rdd).flatMap(lambda x: Contribution(x[1][0], x[1][1]))\n",
    "    for i in range(0,NoOfIterations):\n",
    "        collection = textrank_graph.join(rank_rdd).flatMap(lambda x: Contribution(x[1][0], x[1][1]))\n",
    "        rank_rdd = collection.reduceByKey(lambda x,y: x+y).mapValues(lambda r: 0.15 + 0.85 * r)\n",
    "    sent = \"\"\n",
    "    finalrank = rank_rdd.collect()\n",
    "    count = rank_rdd.count()\n",
    "    if count < 5:\n",
    "        sentence_count = count\n",
    "    else:\n",
    "        sentence_count = 5\n",
    "    result = sorted(finalrank, key=lambda x: x[1], reverse=True)\n",
    "    textfile.collect()\n",
    "    for j in range(0, sentence_count):\n",
    "        sent = sent + textfile.lookup(result[j][0])[0].replace('\\n', \"\")\n",
    "    output = []\n",
    "    output.append(sent)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(1,items+1):\n",
    "    result.append(Summary(rdd.take(i)[i-1]))\n",
    "df = sql.createDataFrame(result, ['GENERATEDSUMMARY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed(\"url\",\"URL\").withColumnRenamed(\"category\", \"CATEGORY\").withColumnRenamed(\"content\", \"CONTENT\").withColumnRenamed(\"summary\", \"ORIGINALSUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  data.withColumn(\"row_num\", row_number().over(w))\n",
    "df =  df.withColumn(\"row_num\", row_number().over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = data.join(df, data.row_num == df.row_num, 'inner').drop(df.row_num)\n",
    "summary = summary.drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextrankSummary = summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextrankSummary.to_csv('TextrankResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = TextrankSummary[\"GENERATEDSUMMARY\"]\n",
    "reference = TextrankSummary[\"ORIGINALSUMMARY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“We must back away from increasing the degree of policy accommodation in a manner commensurate with an improving economy,” Plosser told a panel in Paris and the improving forecast for the near future, Federal Reserve Bank of Philadelphia President Charles Plosser said Monday economy accelerated its pace of expansion in the second half of 2013 from the first halfPARISn — The Federal Reserve may have to accelerate the pace of tapering to take into account the economic pickup currently ongoing in the U At the current pace, the FOMC will end the purchase program later this year'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“We must back away from increasing the degree of policy accommodation in a manner commensurate with an improving economy,” Plosser told a panel in Paris.\\nAs the economic outlook improves, the Fed announced in January its second cut to its monthly purchase program to $65 billion.\\nAt the current pace, the FOMC will end the purchase program later this year.\\nBut Plosser noted the pace may not be fast enough.\\n“Knock on wood it would all go very smoothly, but you never know,” he said in a question-and-answer session after the speech.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "allhypothesis = [''.join(hypothesis[0 : len(hypothesis)-1])]\n",
    "allreference = [''.join(reference[0 : len(reference)-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(list): \n",
    "    return (list[0].split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4055809013469946\n"
     ]
    }
   ],
   "source": [
    "hypothesis_words = convert(hypothesis)\n",
    "reference_words = convert(reference)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference_words], hypothesis_words, weights = (0.5, 0.5))\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48046996885286436\n"
     ]
    }
   ],
   "source": [
    "hypothesis_words = convert(allhypothesis)\n",
    "reference_words = convert(allreference)\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference_words], hypothesis_words, weights = (0.5, 0.5))\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.5065502133483344,\n",
       "   'p': 0.5272727272727272,\n",
       "   'r': 0.48739495798319327},\n",
       "  'rouge-2': {'f': 0.3700440478712958,\n",
       "   'p': 0.3853211009174312,\n",
       "   'r': 0.3559322033898305},\n",
       "  'rouge-l': {'f': 0.45901638844635556,\n",
       "   'p': 0.47191011235955055,\n",
       "   'r': 0.44680851063829785}},\n",
       " {'rouge-1': {'f': 0.7719298197830102,\n",
       "   'p': 0.8777777777777777,\n",
       "   'r': 0.6376811594202898},\n",
       "  'rouge-2': {'f': 0.6345132695618295,\n",
       "   'p': 0.8325842696629213,\n",
       "   'r': 0.5058394160583942},\n",
       "  'rouge-l': {'f': 0.7050314416850601,\n",
       "   'p': 0.6696969696969697,\n",
       "   'r': 0.5881720430107527}},\n",
       " {'rouge-1': {'f': 0.5714285664320709,\n",
       "   'p': 0.5567010309278351,\n",
       "   'r': 0.5869565217391305},\n",
       "  'rouge-2': {'f': 0.41711229446881526,\n",
       "   'p': 0.40625,\n",
       "   'r': 0.42857142857142855},\n",
       "  'rouge-l': {'f': 0.5306122399000418, 'p': 0.5416666666666666, 'r': 0.52}},\n",
       " {'rouge-1': {'f': 0.5444444404320988, 'p': 0.98, 'r': 0.3769230769230769},\n",
       "  'rouge-2': {'f': 0.528089883650423,\n",
       "   'p': 0.7591836734693877,\n",
       "   'r': 0.3643410852713178},\n",
       "  'rouge-l': {'f': 0.5839416016452662,\n",
       "   'p': 0.675609756097561,\n",
       "   'r': 0.4166666666666667}},\n",
       " {'rouge-1': {'f': 0.70975609259536,\n",
       "   'p': 0.6477477477477478,\n",
       "   'r': 0.7829787234042553},\n",
       "  'rouge-2': {'f': 0.6783251181877746,\n",
       "   'p': 0.6181818181818181,\n",
       "   'r': 0.7494623655913979},\n",
       "  'rouge-l': {'f': 0.7201438799047669,\n",
       "   'p': 0.6808219178082192,\n",
       "   'r': 0.7636363636363636}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis[0:5], reference[0:5])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(len(hypothesis_words)*len(reference_words)+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.5707168845634545,\n",
       "   'p': 0.6323255813953488,\n",
       "   'r': 0.5162004175365344},\n",
       "  'rouge-2': {'f': 0.5237515176686879,\n",
       "   'p': 0.6268221574344023,\n",
       "   'r': 0.4497907949790795},\n",
       "  'rouge-l': {'f': 0.6361746312448513,\n",
       "   'p': 0.6916981132075472,\n",
       "   'r': 0.5687732342007435}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(allhypothesis, allreference)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
